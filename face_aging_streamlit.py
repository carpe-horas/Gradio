# streamlit run face_aging_streamlit.py

import streamlit as st
import cv2
import numpy as np
from deepface import DeepFace
from PIL import Image
import time

# Streamlit UI
st.title("ğŸ§‘ AI ê¸°ë°˜ ì–¼êµ´ ë¶„ì„ ì›¹")
st.write("ì›¹ìº ì„ ì‚¬ìš©í•˜ê±°ë‚˜ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì—¬ ì–¼êµ´ì„ ë¶„ì„í•©ë‹ˆë‹¤.")

# ğŸ”¹ ì›¹ìº  & ì—…ë¡œë“œ ì´ë¯¸ì§€ ì €ì¥ ë³€ìˆ˜ ì´ˆê¸°í™”
if "webcam_active" not in st.session_state:
    st.session_state["webcam_active"] = False
if "captured_image" not in st.session_state:
    st.session_state["captured_image"] = None
if "uploaded_image" not in st.session_state:
    st.session_state["uploaded_image"] = None

# ğŸ”¹ ì›¹ìº  ì‹¤í–‰ & ì¢…ë£Œ ë²„íŠ¼ (ë¹„ìœ¨ ìœ ì§€)
col1, col2 = st.columns([0.2, 0.8])

with col1:
    if st.button("ğŸ“· ì›¹ìº  ì‹¤í–‰", key="start_webcam"):
        st.session_state["webcam_active"] = True
        st.session_state["captured_image"] = None  # ê¸°ì¡´ ìº¡ì²˜ ì´ë¯¸ì§€ ì‚­ì œ
        st.session_state["uploaded_image"] = None  # ê¸°ì¡´ ì—…ë¡œë“œ ì´ë¯¸ì§€ ì‚­ì œ

with col2:
    if st.button("ğŸ›‘ ì›¹ìº  ì¢…ë£Œ", key="stop_webcam"):
        st.session_state["webcam_active"] = False

# ğŸ”¹ ì›¹ìº  ì‹¤í–‰ (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°)
if st.session_state["webcam_active"]:
    video_capture = cv2.VideoCapture(0)

    if not video_capture.isOpened():
        st.error("âŒ ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì—ì„œ ì‚¬ìš© ì¤‘ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        st.session_state["webcam_active"] = False
    else:
        stframe = st.empty()  # ğŸš€ ì‹¤ì‹œê°„ í”„ë ˆì„ ì—…ë°ì´íŠ¸ ê³µê°„
        capture_button_placeholder = st.empty()  # ğŸš€ ìº¡ì²˜ ë²„íŠ¼ ê³µê°„ ìœ ì§€

        # "ğŸ“¸ ìº¡ì²˜" ë²„íŠ¼ì„ while ë£¨í”„ ë°–ì—ì„œ í•œ ë²ˆë§Œ ìƒì„±
        capture_clicked = capture_button_placeholder.button("ğŸ“¸ ìº¡ì²˜", key="capture_button")

        while st.session_state["webcam_active"]:
            ret, frame = video_capture.read()
            if not ret:
                st.error("âŒ ì›¹ìº  í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break

            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            stframe.image(frame, channels="RGB", use_container_width=True)  # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ìœ ì§€

            # "ğŸ“¸ ìº¡ì²˜" ë²„íŠ¼ì´ í´ë¦­ë˜ë©´ í˜„ì¬ í™”ë©´ ì €ì¥ í›„ ì¢…ë£Œ
            if capture_clicked:
                st.session_state["captured_image"] = Image.fromarray(frame)  # ì›¹ìº  ì´ë¯¸ì§€ ì €ì¥
                st.session_state["uploaded_image"] = None  # ê¸°ì¡´ ì—…ë¡œë“œ ì´ë¯¸ì§€ ì‚­ì œ
                st.session_state["webcam_active"] = False  # ìº¡ì²˜ í›„ ì›¹ìº  ì¢…ë£Œ
                st.rerun()  # UI ìƒˆë¡œê³ ì¹¨í•˜ì—¬ ì›¹ìº  ì¢…ë£Œ ë° ì´ë¯¸ì§€ í‘œì‹œ

            time.sleep(0.03)  # í”„ë ˆì„ ì†ë„ ì¡°ì ˆ (CPU ë¶€í•˜ ë°©ì§€)

        video_capture.release()

# **ì—…ë¡œë“œí•œ ì´ë¯¸ì§€ ì²˜ë¦¬ (ì›¹ìº  ì‹¤í–‰ ì‹œ ìë™ ì‚­ì œ)**
uploaded_file = st.file_uploader("ğŸ“‚ ì´ë¯¸ì§€ ì—…ë¡œë“œ (JPG, PNG)", type=["jpg", "png", "jpeg"])
if uploaded_file:
    st.session_state["uploaded_image"] = Image.open(uploaded_file)  # ì—…ë¡œë“œëœ ì´ë¯¸ì§€ ì €ì¥
    st.session_state["captured_image"] = None  # ê¸°ì¡´ ìº¡ì²˜ ì´ë¯¸ì§€ ì‚­ì œ

# ìµœì¢… ë¶„ì„í•  ì–¼êµ´ ì„¤ì • (ì›¹ìº  ìº¡ì²˜ëœ ì´ë¯¸ì§€ê°€ ìš°ì„ )
image_to_analyze = st.session_state["captured_image"] or st.session_state["uploaded_image"]

# ë¶„ì„í•  ì–¼êµ´ ì´ë¯¸ì§€ í‘œì‹œ
if image_to_analyze:
    st.image(image_to_analyze, caption="ğŸ“· ë¶„ì„í•  ì–¼êµ´", width=250)

# ì´ë¯¸ì§€ ë¶„ì„ í•¨ìˆ˜ (ì„±ë³„ í™•ë¥  ë³€í™˜ í¬í•¨)
def analyze_face(image):
    image = np.array(image)
    try:
        analysis = DeepFace.analyze(image, actions=['age', 'gender', 'emotion', 'race'], enforce_detection=False)

        # ì„±ë³„ í™•ë¥  ë³€í™˜ (ë°±ë¶„ìœ¨ í˜•íƒœ)
        gender_probs = analysis[0]['gender']
        male_prob = round(gender_probs['Man'], 2)
        female_prob = round(gender_probs['Woman'], 2)

        # í™•ë¥ ì´ ë†’ì€ ê°’ì„ ìµœì¢… ì„±ë³„ë¡œ í‘œì‹œ
        final_gender = "ë‚¨ì„±" if male_prob > female_prob else "ì—¬ì„±"

        return {
            "age": analysis[0]['age'],
            "gender": final_gender,
            "gender_prob": f"ì—¬ì„±: {female_prob}% | ë‚¨ì„±: {male_prob}%",
            "emotion": max(analysis[0]['emotion'], key=analysis[0]['emotion'].get),
            "race": max(analysis[0]['race'], key=analysis[0]['race'].get)
        }
    except Exception as e:
        st.error("ì–¼êµ´ì„ ê°ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ ì£¼ì„¸ìš”.")
        return None

# ë¶„ì„ ë²„íŠ¼
# if image_to_analyze:
#     if st.button("ì–¼êµ´ ë¶„ì„í•˜ê¸°", key="analyze_button"):
#         analysis = analyze_face(image_to_analyze)
#         if analysis:
#             st.success(f"ğŸ”¢ **ì˜ˆìƒ ë‚˜ì´:** {analysis['age']}ì„¸")
#             st.success(f"âš¤ **ì„±ë³„:** {analysis['gender']} ({analysis['gender_prob']})")
#             st.success(f"ğŸ™‚ **ê°ì • ìƒíƒœ:** {analysis['emotion']}")
#             st.success(f"ğŸŒ **ì¸ì¢…:** {analysis['race']}")

if image_to_analyze:
    if st.button("ì–¼êµ´ ë¶„ì„í•˜ê¸°", key="analyze_button"):
        analysis = analyze_face(image_to_analyze)
        if analysis:
            result_text = f"""
            <div style="padding: 10px; border-radius: 10px; background-color: #fcf0fb; border: 1px solid #f7c1f2;">
                <h4>ğŸ” ë¶„ì„ ê²°ê³¼</h4>
                <p><b>ğŸ”¢ ì˜ˆìƒ ë‚˜ì´:</b> {analysis['age']}ì„¸</p>
                <p><b>âš¤ ì„±ë³„:</b> {analysis['gender']} ({analysis['gender_prob']})</p>
                <p><b>ğŸ™‚ ê°ì • ìƒíƒœ:</b> {analysis['emotion']}</p>
                <p><b>ğŸŒ ì¸ì¢…:</b> {analysis['race']}</p>
            </div>
            """
            st.markdown(result_text, unsafe_allow_html=True)

